{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "TensorflowGPU",
      "language": "python",
      "name": "tensorflow_gpuenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Modelling.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DrkowguVxzg",
        "outputId": "aefcac88-0c15-4eb5-dd8c-8925afcfd81c"
      },
      "source": [
        "!pip install fuzzywuzzy distance"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.7/dist-packages (0.18.0)\n",
            "Requirement already satisfied: distance in /usr/local/lib/python3.7/dist-packages (0.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toA9oMAJSRpo"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Tools for preprocessing input data\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "import seaborn as sns\n",
        "# Tools for creating ngrams and vectorizing input data\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from gensim.models import KeyedVectors\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import PorterStemmer\n",
        "#from bs4 import BeautifulSoup\n",
        "from fuzzywuzzy import fuzz\n",
        "import distance\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import Dense, Embedding,Bidirectional,LSTM,Attention,Concatenate,GlobalAveragePooling1D,Dropout\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from keras.layers.normalization import BatchNormalization"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81tXyrR0VQEN",
        "outputId": "0b8061f3-8a5c-4423-cbff-21d154bb2dbf"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUzCBWFQSRp8"
      },
      "source": [
        "train_df=pd.read_csv('/content/gdrive/MyDrive/Kaggle/train.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg9MfeWLSRp9",
        "outputId": "ea8a4d9b-deab-4857-8703-6690fb534649"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(404290, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbGdMGlISRqC"
      },
      "source": [
        "# Filling the null values with ' '\n",
        "train_df = train_df.fillna('')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qUPV51oSRqC"
      },
      "source": [
        "# Calculate question length and n_words\n",
        "train_df['q1len'] = train_df['question1'].str.len() \n",
        "train_df['q2len'] = train_df['question2'].str.len()\n",
        "train_df['q1_n_words'] = train_df['question1'].apply(lambda row: len(row.split(\" \")))\n",
        "train_df['q2_n_words'] = train_df['question2'].apply(lambda row: len(row.split(\" \")))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXlQVZpNSRqD"
      },
      "source": [
        "def preprocessing(text, remove_stopwords=False, stem_words=False):\n",
        "    # Clean the text, with the option to remove stopwords and to stem words.\n",
        "    \n",
        "    # Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stopwords:\n",
        "        stop_words = set(stopwords.words(\"english\"))\n",
        "        text = [w for w in text if not w in stop_words]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    \n",
        "    # Remove punctuation from text\n",
        "    # text = \"\".join([c for c in text if c not in punctuation])\n",
        "\n",
        "    # Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"can't\", \"cannot \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text = text.split()\n",
        "        stemmer = SnowballStemmer('english')\n",
        "        stemmed_words = [stemmer.stem(word) for word in text]\n",
        "        text = \" \".join(stemmed_words)\n",
        "    \n",
        "    # Return a list of words\n",
        "    return(text)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noDpI6DHSRqG"
      },
      "source": [
        "train_df['question1_clean'] = train_df['question1'].apply(preprocessing)\n",
        "train_df['question2_clean'] = train_df['question2'].apply(preprocessing)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVYTL6FbSRqH"
      },
      "source": [
        "# Extract_Feautres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PSg3X_0SRqI"
      },
      "source": [
        "# get the Longest Common sub string\n",
        "\n",
        "def get_longest_substr_ratio(a, b):\n",
        "    strs = list(distance.lcsubstrings(a, b))\n",
        "    if len(strs) == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
        "\n",
        "# Extract fuzzy-wuzzy features\n",
        "def extract_features (df):\n",
        "    # The token sort approach involves tokenizing the string in question, sorting the tokens alphabetically, and \n",
        "    # then joining them back into a string We then compare the transformed strings with a simple ratio().\n",
        "    df[\"token_sort_ratio\"]      = df.apply(lambda x: fuzz.token_sort_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_ratio\"]            = df.apply(lambda x: fuzz.QRatio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"fuzz_partial_ratio\"]    = df.apply(lambda x: fuzz.partial_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    df[\"longest_substr_ratio\"]  = df.apply(lambda x: get_longest_substr_ratio(x[\"question1\"], x[\"question2\"]), axis=1)\n",
        "    return df\n",
        "\n",
        "def normalized_word_share(row):\n",
        "    w1 = set(map(lambda word: word.lower().strip().replace('?', ''), row['question1'].split(\" \")))\n",
        "    w2 = set(map(lambda word: word.lower().strip().replace('?', ''), row['question2'].split(\" \")))    \n",
        "    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Eh8b353SRqK"
      },
      "source": [
        "train_df = extract_features(train_df)\n",
        "train_df['word_share'] = train_df.apply(normalized_word_share, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jf6eI0cSRqK"
      },
      "source": [
        "for word in ['who', 'how', 'where', 'why', 'what', 'when', 'which']:\n",
        "    train_df['have'+word+\"_1\"] = False\n",
        "    train_df['have'+word+\"_2\"] = False\n",
        "    train_df.loc[train_df.question1_clean.str.contains(r\"\\b\"+word+\"\\\\b\", regex=True), ['have'+word+\"_1\"]] = True\n",
        "    train_df.loc[train_df.question2_clean.str.contains(r\"\\b\"+word+\"\\\\b\", regex=True), ['have'+word+\"_2\"]] = True\n",
        "    train_df.loc[train_df.question1_clean.str.contains(r\"\\b\"+word+\"\\\\b\", regex=True), [\"question_word\"+\"_1\"]] = word\n",
        "    train_df.loc[train_df.question2_clean.str.contains(r\"\\b\"+word+\"\\\\b\", regex=True), [\"question_word\"+\"_2\"]] = word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHiUZPoSRqN"
      },
      "source": [
        "train_df['have_same_q_w'] = False\n",
        "train_df.loc[(train_df.question_word_1==train_df.question_word_2),['have_same_q_w']] = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siKA2DqSRqO"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKTQz2MRSRqP"
      },
      "source": [
        "#train_df.to_csv('train_preprocessed.csv', index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijnw0VKTSRqQ"
      },
      "source": [
        "max_num_words = 200000\n",
        "sequence_length = 60\n",
        "embedding_dim = 300\n",
        "validation_split_ratio = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2QFNEE5SRqR"
      },
      "source": [
        "q1 = train_df['question1_clean'].values\n",
        "q2 = train_df['question2_clean'].values\n",
        "tokenizer = Tokenizer(num_words = max_num_words, split=' ', oov_token='<unw>')\n",
        "tokenizer.fit_on_texts(q1+q2)\n",
        "\n",
        "# this takes our sentences and replaces each word with an integer\n",
        "X_1 = tokenizer.texts_to_sequences(q1)\n",
        "X_2 = tokenizer.texts_to_sequences(q2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAIowSrjSRqT"
      },
      "source": [
        "# we then pad the sequences so they're all the same length (sequence_length)\n",
        "X_1 = pad_sequences(X_1, int(sequence_length))\n",
        "X_2 = pad_sequences(X_2, int(sequence_length))\n",
        "\n",
        "y = train_df['is_duplicate']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBfiPhwrSRqU"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzskL3uoaTkC"
      },
      "source": [
        "import pickle\r\n",
        "embeddings_index = pickle.load( open( \"/content/gdrive/MyDrive/embedding_matrix.p\", \"rb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvE6qfJ2SRqX"
      },
      "source": [
        "  embedding_matrix = np.zeros((max_num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZrldVqZSRqZ"
      },
      "source": [
        "X = list(zip(X_1, X_2, train_df['q1_n_words'].values, train_df['q2_n_words'].values, train_df['have_same_q_w'].values ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OikDdtDlSRqa"
      },
      "source": [
        "import pickle\n",
        "pickle.dump( X, open( \"X.p\", \"wb\" ) )\n",
        "pickle.dump( y, open( \"y.p\", \"wb\" ) )\n",
        "pickle.dump( embedding_matrix, open( \"embedding_matrix.p\", \"wb\" ) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGomP5xHSRqa"
      },
      "source": [
        "q1 = Input(shape=(sequence_length,))\n",
        "q2 = Input(shape=(sequence_length,))\n",
        "q1_n_words = Input(shape=(1,))\n",
        "q2_length = Input(shape=(1,))\n",
        "emb = Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1], \n",
        "                            weights=[embedding_matrix],input_length = sequence_length, trainable=False)\n",
        "lstm = Bidirectional(LSTM(256 ,dropout=0.5, return_sequences = True))\n",
        "\n",
        "e1 = emb(q1)\n",
        "l1 = lstm(e1)\n",
        "\n",
        "e2 = emb(q2)\n",
        "l2 = lstm(e2)\n",
        "\n",
        "atten_1 = Attention(sequence_length)(l1)\n",
        "atten_2 = Attention(sequence_length)(l2)\n",
        "concat = Concatenate()([atten_1,atten_2,q1_length, q2_length])\n",
        "output = Dense(300, activation=\"relu\")(concat)\n",
        "output = BatchNormalization()(concat)\n",
        "output = Dropout(0.3)\n",
        "output = Dense(150, activation=\"relu\")(concat)\n",
        "\n",
        "output = Dense(1, activation=\"sigmoid\")(output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SAdn8dASRqb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}